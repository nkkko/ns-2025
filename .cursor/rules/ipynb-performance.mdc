---
description: 
globs: 
alwaysApply: false
---
name: "Jupyter Notebook Performance"
description: "Best practices for optimizing performance in Jupyter notebooks"
tags: ["jupyter", "python", "notebook", "performance", "optimization"]
fileMatch: ["**/*.ipynb"]
severity: "suggestion"

rule: |
  # Performance Optimization in Jupyter Notebooks

  ## Memory Management
  - Use generators for large data processing when possible
  - Delete large objects when no longer needed (`del variable_name`)
  - Consider using memory-efficient data structures (e.g., NumPy arrays vs. lists)
  - Monitor memory usage with tools like `%memit` from `memory_profiler`
  - Be cautious with operations that create copies of large datasets

  ## Computation Efficiency
  - Use vectorized operations instead of loops when possible
  - Consider using `numba`, `cython`, or `joblib` for compute-intensive tasks
  - Implement caching for expensive computations
  - Use appropriate data types to minimize memory usage
  - Consider downsampling large datasets for exploratory analysis

  ## Execution Time
  - Use `%%time` or `%%timeit` magic commands to measure cell execution time
  - Profile code with `%%prun` or `cProfile` for identifying bottlenecks
  - Consider using progress bars (`tqdm`) for long-running operations
  - Break complex computations into smaller, checkpointed steps
  - Save intermediate results for time-consuming processes

  ## Parallel Processing
  - Use multiprocessing for CPU-bound tasks
  - Consider using `dask` for parallel and out-of-memory computations
  - Leverage GPU acceleration when appropriate (e.g., with `cupy`, `tensorflow`, `pytorch`)
  - Be mindful of the overhead of parallelization for small tasks
  - Document any environment-specific parallel processing configurations